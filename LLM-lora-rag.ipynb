{
 "metadata": {
  "kaggle": {
   "accelerator": "tpu1vmV38",
   "dataSources": [
    {
     "sourceId": 64148,
     "databundleVersionId": 7669720,
     "sourceType": "competition"
    },
    {
     "sourceId": 928135,
     "sourceType": "datasetVersion",
     "datasetId": 501056
    },
    {
     "sourceId": 7711309,
     "sourceType": "datasetVersion",
     "datasetId": 4484051
    },
    {
     "sourceId": 7870394,
     "sourceType": "datasetVersion",
     "datasetId": 4618087
    },
    {
     "sourceId": 141343065,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 11372,
     "sourceType": "modelInstanceVersion",
     "modelInstanceId": 5388
    }
   ],
   "dockerImageVersionId": 30647,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": false
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Install all the packages. Be sure to use accelerator TPU."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install tensorflow-cpu tensorflow-hub tensorflow-text\n",
    "!pip install  Pyarrow\n",
    "!pip install langchain\n",
    "!pip install --quiet langchain_experimental langchain_openai\n",
    "!pip install pypdf\n",
    "# Install Keras 3 last. See https://keras.io/getting_started/ for more details.\n",
    "!pip install -q -U keras-nlp\n",
    "!pip install -q -U keras>=3\n",
    "\n",
    "!pip install sentence_transformers\n",
    "!pip install chromadb"
   ],
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.status.busy": "2024-04-13T21:59:33.230918Z",
     "iopub.execute_input": "2024-04-13T21:59:33.231390Z",
     "iopub.status.idle": "2024-04-13T22:02:44.189115Z",
     "shell.execute_reply.started": "2024-04-13T21:59:33.231358Z",
     "shell.execute_reply": "2024-04-13T22:02:44.188113Z"
    },
    "trusted": true
   },
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "text": "Collecting tensorflow-cpu\n  Downloading tensorflow_cpu-2.16.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (214.0 MB)\n\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m214.0/214.0 MB\u001B[0m \u001B[31m3.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\n\u001B[?25hRequirement already satisfied: tensorflow-hub in /usr/local/lib/python3.10/site-packages (0.16.0)\nRequirement already satisfied: tensorflow-text in /usr/local/lib/python3.10/site-packages (2.15.0)\nRequirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/site-packages (from tensorflow-cpu) (16.0.6)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.10/site-packages (from tensorflow-cpu) (65.5.1)\nCollecting tensorboard<2.17,>=2.16\n  Downloading tensorboard-2.16.2-py3-none-any.whl (5.5 MB)\n\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m5.5/5.5 MB\u001B[0m \u001B[31m64.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m:00:01\u001B[0m00:01\u001B[0m\n\u001B[?25hRequirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/site-packages (from tensorflow-cpu) (0.5.4)\nRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/site-packages (from tensorflow-cpu) (3.20.3)\nRequirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/site-packages (from tensorflow-cpu) (1.16.0)\nRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/site-packages (from tensorflow-cpu) (2.4.0)\nRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/site-packages (from tensorflow-cpu) (1.4.0)\nRequirement already satisfied: keras>=3.0.0 in /usr/local/lib/python3.10/site-packages (from tensorflow-cpu) (3.0.4)\nRequirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/site-packages (from tensorflow-cpu) (0.2.0)\nRequirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/site-packages (from tensorflow-cpu) (1.6.3)\nRequirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/site-packages (from tensorflow-cpu) (3.10.0)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/site-packages (from tensorflow-cpu) (1.60.0)\nRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/site-packages (from tensorflow-cpu) (3.3.0)\nRequirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/site-packages (from tensorflow-cpu) (23.5.26)\nRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/site-packages (from tensorflow-cpu) (0.35.0)\nRequirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/site-packages (from tensorflow-cpu) (2.31.0)\nCollecting ml-dtypes~=0.3.1\n  Downloading ml_dtypes-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m2.2/2.2 MB\u001B[0m \u001B[31m62.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m:00:01\u001B[0m\n\u001B[?25hRequirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/site-packages (from tensorflow-cpu) (4.9.0)\nRequirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/site-packages (from tensorflow-cpu) (1.14.1)\nRequirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/site-packages (from tensorflow-cpu) (1.26.3)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/site-packages (from tensorflow-cpu) (23.2)\nRequirement already satisfied: tensorflow<2.16,>=2.15.0 in /usr/local/lib/python3.10/site-packages (from tensorflow-text) (2.15.0)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow-cpu) (0.42.0)\nRequirement already satisfied: rich in /usr/local/lib/python3.10/site-packages (from keras>=3.0.0->tensorflow-cpu) (13.7.0)\nRequirement already satisfied: dm-tree in /usr/local/lib/python3.10/site-packages (from keras>=3.0.0->tensorflow-cpu) (0.1.8)\nCollecting namex\n  Downloading namex-0.0.7-py3-none-any.whl (5.8 kB)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow-cpu) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow-cpu) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow-cpu) (2.1.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow-cpu) (2023.11.17)\nRequirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/site-packages (from tensorboard<2.17,>=2.16->tensorflow-cpu) (3.5.2)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/site-packages (from tensorboard<2.17,>=2.16->tensorflow-cpu) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/site-packages (from tensorboard<2.17,>=2.16->tensorflow-cpu) (3.0.1)\nCollecting tensorflow<2.16,>=2.15.0\n  Downloading tensorflow-2.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (475.2 MB)\n\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m475.2/475.2 MB\u001B[0m \u001B[31m886.2 kB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\n\u001B[?25h  Downloading tensorflow-2.15.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (475.2 MB)\n\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m475.2/475.2 MB\u001B[0m \u001B[31m1.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\n\u001B[?25hINFO: pip is looking at multiple versions of tensorboard to determine which version is compatible with other requirements. This could take a while.\nCollecting tensorboard<2.17,>=2.16\n  Downloading tensorboard-2.16.1-py3-none-any.whl (5.5 MB)\n\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m5.5/5.5 MB\u001B[0m \u001B[31m51.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\n\u001B[?25hINFO: pip is looking at multiple versions of six to determine which version is compatible with other requirements. This could take a while.\nCollecting six>=1.12.0\n  Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)\nINFO: pip is looking at multiple versions of requests to determine which version is compatible with other requirements. This could take a while.\nCollecting requests<3,>=2.21.0\n  Downloading requests-2.31.0-py3-none-any.whl (62 kB)\n\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m62.6/62.6 kB\u001B[0m \u001B[31m6.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hINFO: pip is looking at multiple versions of protobuf to determine which version is compatible with other requirements. This could take a while.\nCollecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3\n  Downloading protobuf-4.25.3-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m294.6/294.6 kB\u001B[0m \u001B[31m23.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hINFO: pip is looking at multiple versions of opt-einsum to determine which version is compatible with other requirements. This could take a while.\nCollecting opt-einsum>=2.3.2\n  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m65.5/65.5 kB\u001B[0m \u001B[31m5.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hINFO: pip is looking at multiple versions of numpy to determine which version is compatible with other requirements. This could take a while.\nCollecting numpy<2.0.0,>=1.23.5\n  Downloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m18.2/18.2 MB\u001B[0m \u001B[31m47.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\n\u001B[?25hINFO: pip is looking at multiple versions of ml-dtypes to determine which version is compatible with other requirements. This could take a while.\nCollecting ml-dtypes~=0.3.1\n  Downloading ml_dtypes-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (206 kB)\n\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m206.7/206.7 kB\u001B[0m \u001B[31m17.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hINFO: pip is looking at multiple versions of libclang to determine which version is compatible with other requirements. This could take a while.\nCollecting libclang>=13.0.0\n  Downloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m24.5/24.5 MB\u001B[0m \u001B[31m39.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\n\u001B[?25hINFO: pip is looking at multiple versions of keras to determine which version is compatible with other requirements. This could take a while.\nCollecting keras>=3.0.0\n  Downloading keras-3.2.1-py3-none-any.whl (1.1 MB)\n\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.1/1.1 MB\u001B[0m \u001B[31m52.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hCollecting optree\n  Downloading optree-0.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (311 kB)\n\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m311.2/311.2 kB\u001B[0m \u001B[31m23.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hCollecting keras>=3.0.0\n  Downloading keras-3.2.0-py3-none-any.whl (1.1 MB)\n\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.1/1.1 MB\u001B[0m \u001B[31m46.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25h  Downloading keras-3.1.1-py3-none-any.whl (1.1 MB)\n\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.1/1.1 MB\u001B[0m \u001B[31m32.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25h  Downloading keras-3.1.0-py3-none-any.whl (1.1 MB)\n\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.1/1.1 MB\u001B[0m \u001B[31m47.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25h  Downloading keras-3.0.5-py3-none-any.whl (1.0 MB)\n\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.0/1.0 MB\u001B[0m \u001B[31m49.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25h  Downloading keras-3.0.4-py3-none-any.whl (1.0 MB)\n\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.0/1.0 MB\u001B[0m \u001B[31m12.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\n\u001B[?25h  Downloading keras-3.0.3-py3-none-any.whl (1.0 MB)\n\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.0/1.0 MB\u001B[0m \u001B[31m47.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hINFO: pip is looking at multiple versions of keras to determine which version is compatible with other requirements. This could take a while.\n  Downloading keras-3.0.2-py3-none-any.whl (1.0 MB)\n\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.0/1.0 MB\u001B[0m \u001B[31m45.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25h  Downloading keras-3.0.1-py3-none-any.whl (999 kB)\n\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m999.1/999.1 kB\u001B[0m \u001B[31m45.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25h  Downloading keras-3.0.0-py3-none-any.whl (997 kB)\n\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m997.1/997.1 kB\u001B[0m \u001B[31m12.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\n\u001B[?25hINFO: pip is looking at multiple versions of h5py to determine which version is compatible with other requirements. This could take a while.\nCollecting h5py>=3.10.0\n  Downloading h5py-3.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.3 MB)\n\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m5.3/5.3 MB\u001B[0m \u001B[31m64.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m:00:01\u001B[0m00:01\u001B[0m\n\u001B[?25hINFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\nINFO: pip is looking at multiple versions of grpcio to determine which version is compatible with other requirements. This could take a while.\nCollecting grpcio<2.0,>=1.24.3\n  Downloading grpcio-1.62.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.5 MB)\n\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m5.5/5.5 MB\u001B[0m \u001B[31m68.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m:00:01\u001B[0m00:01\u001B[0m\n\u001B[?25hINFO: pip is looking at multiple versions of google-pasta to determine which version is compatible with other requirements. This could take a while.\nCollecting google-pasta>=0.1.1\n  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m57.5/57.5 kB\u001B[0m \u001B[31m4.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hINFO: pip is looking at multiple versions of gast to determine which version is compatible with other requirements. This could take a while.\nCollecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1\n  Downloading gast-0.5.4-py3-none-any.whl (19 kB)\nINFO: pip is looking at multiple versions of flatbuffers to determine which version is compatible with other requirements. This could take a while.\nCollecting flatbuffers>=23.5.26\n  Downloading flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\nINFO: pip is looking at multiple versions of astunparse to determine which version is compatible with other requirements. This could take a while.\nCollecting astunparse>=1.6.0\n  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\nINFO: pip is looking at multiple versions of absl-py to determine which version is compatible with other requirements. This could take a while.\nCollecting absl-py>=1.0.0\n  Downloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m133.7/133.7 kB\u001B[0m \u001B[31m12.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hINFO: pip is looking at multiple versions of tensorflow-text to determine which version is compatible with other requirements. This could take a while.\nCollecting tensorflow-text\n  Downloading tensorflow_text-2.16.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.2 MB)\n\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m5.2/5.2 MB\u001B[0m \u001B[31m66.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m:00:01\u001B[0m00:01\u001B[0m\n\u001B[?25hCollecting tensorflow<2.17,>=2.16.1\n  Downloading tensorflow-2.16.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (589.8 MB)\n\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m589.8/589.8 MB\u001B[0m \u001B[31m739.0 kB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\n\u001B[?25hRequirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow-cpu) (2.1.4)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/site-packages (from rich->keras>=3.0.0->tensorflow-cpu) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/site-packages (from rich->keras>=3.0.0->tensorflow-cpu) (2.17.2)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow-cpu) (0.1.2)\nInstalling collected packages: namex, ml-dtypes, tensorboard, tensorflow-cpu, tensorflow, tensorflow-text\n  Attempting uninstall: ml-dtypes\n    Found existing installation: ml-dtypes 0.2.0\n    Uninstalling ml-dtypes-0.2.0:\n      Successfully uninstalled ml-dtypes-0.2.0\n  Attempting uninstall: tensorboard\n    Found existing installation: tensorboard 2.15.1\n    Uninstalling tensorboard-2.15.1:\n      Successfully uninstalled tensorboard-2.15.1\n  Attempting uninstall: tensorflow\n    Found existing installation: tensorflow 2.15.0\n    Uninstalling tensorflow-2.15.0:\n      Successfully uninstalled tensorflow-2.15.0\n  Attempting uninstall: tensorflow-text\n    Found existing installation: tensorflow-text 2.15.0\n    Uninstalling tensorflow-text-2.15.0:\n      Successfully uninstalled tensorflow-text-2.15.0\n\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nkeras-nlp 0.7.0 requires keras-core, which is not installed.\u001B[0m\u001B[31m\n\u001B[0mSuccessfully installed ml-dtypes-0.3.2 namex-0.0.7 tensorboard-2.16.2 tensorflow-2.16.1 tensorflow-cpu-2.16.1 tensorflow-text-2.16.1\n\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n\u001B[0m\n\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.0.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m24.0\u001B[0m\n\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\nCollecting Pyarrow\n  Downloading pyarrow-15.0.2-cp310-cp310-manylinux_2_28_x86_64.whl (38.3 MB)\n\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m38.3/38.3 MB\u001B[0m \u001B[31m21.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\n\u001B[?25hRequirement already satisfied: numpy<2,>=1.16.6 in /usr/local/lib/python3.10/site-packages (from Pyarrow) (1.26.3)\nInstalling collected packages: Pyarrow\nSuccessfully installed Pyarrow-15.0.2\n\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n\u001B[0m\n\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.0.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m24.0\u001B[0m\n\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\nCollecting langchain\n  Downloading langchain-0.1.16-py3-none-any.whl (817 kB)\n\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m817.7/817.7 kB\u001B[0m \u001B[31m3.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\n\u001B[?25hCollecting jsonpatch<2.0,>=1.33\n  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\nCollecting langchain-community<0.1,>=0.0.32\n  Downloading langchain_community-0.0.32-py3-none-any.whl (1.9 MB)\n\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.9/1.9 MB\u001B[0m \u001B[31m24.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\n\u001B[?25hCollecting SQLAlchemy<3,>=1.4\n  Downloading SQLAlchemy-2.0.29-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m3.1/3.1 MB\u001B[0m \u001B[31m46.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0mta \u001B[36m0:00:01\u001B[0m\n\u001B[?25hCollecting langsmith<0.2.0,>=0.1.17\n  Downloading langsmith-0.1.47-py3-none-any.whl (113 kB)\n\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m113.0/113.0 kB\u001B[0m \u001B[31m10.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hRequirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/site-packages (from langchain) (8.2.3)\nCollecting langchain-text-splitters<0.1,>=0.0.1\n  Downloading langchain_text_splitters-0.0.1-py3-none-any.whl (21 kB)\nCollecting async-timeout<5.0.0,>=4.0.0\n  Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\nCollecting aiohttp<4.0.0,>=3.8.3\n  Downloading aiohttp-3.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.2/1.2 MB\u001B[0m \u001B[31m1.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m0:00:01\u001B[0m\n\u001B[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/site-packages (from langchain) (6.0.1)\nCollecting langchain-core<0.2.0,>=0.1.42\n  Downloading langchain_core-0.1.42-py3-none-any.whl (287 kB)\n\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m287.5/287.5 kB\u001B[0m \u001B[31m3.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m\n\u001B[?25hCollecting pydantic<3,>=1\n  Downloading pydantic-2.7.0-py3-none-any.whl (407 kB)\n\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m407.9/407.9 kB\u001B[0m \u001B[31m3.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m\n\u001B[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/site-packages (from langchain) (1.26.3)\nCollecting dataclasses-json<0.7,>=0.5.7\n  Downloading dataclasses_json-0.6.4-py3-none-any.whl (28 kB)\nRequirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/site-packages (from langchain) (2.31.0)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\nCollecting yarl<2.0,>=1.0\n  Downloading yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (301 kB)\n\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m301.6/301.6 kB\u001B[0m \u001B[31m3.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m\n\u001B[?25hCollecting frozenlist>=1.1.1\n  Downloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (239 kB)\n\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m239.5/239.5 kB\u001B[0m \u001B[31m17.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hCollecting aiosignal>=1.1.2\n  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\nCollecting multidict<7.0,>=4.5\n  Downloading multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\n\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m124.3/124.3 kB\u001B[0m \u001B[31m11.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hCollecting marshmallow<4.0.0,>=3.18.0\n  Downloading marshmallow-3.21.1-py3-none-any.whl (49 kB)\n\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m49.4/49.4 kB\u001B[0m \u001B[31m5.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hCollecting typing-inspect<1,>=0.4.0\n  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\nRequirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\nRequirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/site-packages (from langchain-core<0.2.0,>=0.1.42->langchain) (23.2)\nCollecting orjson<4.0.0,>=3.9.14\n  Downloading orjson-3.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m144.8/144.8 kB\u001B[0m \u001B[31m13.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hCollecting pydantic-core==2.18.1\n  Downloading pydantic_core-2.18.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m2.1/2.1 MB\u001B[0m \u001B[31m57.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m:00:01\u001B[0m\n\u001B[?25hCollecting annotated-types>=0.4.0\n  Downloading annotated_types-0.6.0-py3-none-any.whl (12 kB)\nRequirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/site-packages (from pydantic<3,>=1->langchain) (4.9.0)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.6)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2023.11.17)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.3.2)\nCollecting greenlet!=0.4.17\n  Downloading greenlet-3.0.3-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (616 kB)\n\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m616.0/616.0 kB\u001B[0m \u001B[31m36.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hCollecting mypy-extensions>=0.3.0\n  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\nInstalling collected packages: pydantic-core, orjson, mypy-extensions, multidict, marshmallow, jsonpatch, greenlet, frozenlist, async-timeout, annotated-types, yarl, typing-inspect, SQLAlchemy, pydantic, aiosignal, langsmith, dataclasses-json, aiohttp, langchain-core, langchain-text-splitters, langchain-community, langchain\nSuccessfully installed SQLAlchemy-2.0.29 aiohttp-3.9.4 aiosignal-1.3.1 annotated-types-0.6.0 async-timeout-4.0.3 dataclasses-json-0.6.4 frozenlist-1.4.1 greenlet-3.0.3 jsonpatch-1.33 langchain-0.1.16 langchain-community-0.0.32 langchain-core-0.1.42 langchain-text-splitters-0.0.1 langsmith-0.1.47 marshmallow-3.21.1 multidict-6.0.5 mypy-extensions-1.0.0 orjson-3.10.0 pydantic-2.7.0 pydantic-core-2.18.1 typing-inspect-0.9.0 yarl-1.9.4\n\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n\u001B[0m\n\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.0.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m24.0\u001B[0m\n\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\n\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n\u001B[0m\n\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.0.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m24.0\u001B[0m\n\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\nCollecting pypdf\n  Downloading pypdf-4.2.0-py3-none-any.whl (290 kB)\n\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m290.4/290.4 kB\u001B[0m \u001B[31m2.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\n\u001B[?25hRequirement already satisfied: typing_extensions>=4.0 in /usr/local/lib/python3.10/site-packages (from pypdf) (4.9.0)\nInstalling collected packages: pypdf\nSuccessfully installed pypdf-4.2.0\n\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n\u001B[0m\n\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.0.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m24.0\u001B[0m\n\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\n\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n\u001B[0m\n\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.0.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m24.0\u001B[0m\n\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\n\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n\u001B[0m\n\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.0.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m24.0\u001B[0m\n\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\nCollecting sentence_transformers\n  Downloading sentence_transformers-2.6.1-py3-none-any.whl (163 kB)\n\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m163.3/163.3 kB\u001B[0m \u001B[31m1.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\n\u001B[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/site-packages (from sentence_transformers) (4.66.1)\nRequirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/site-packages (from sentence_transformers) (0.20.3)\nRequirement already satisfied: transformers<5.0.0,>=4.32.0 in /usr/local/lib/python3.10/site-packages (from sentence_transformers) (4.37.0)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.10/site-packages (from sentence_transformers) (1.4.0)\nRequirement already satisfied: scipy in /usr/local/lib/python3.10/site-packages (from sentence_transformers) (1.12.0)\nRequirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/site-packages (from sentence_transformers) (2.1.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/site-packages (from sentence_transformers) (1.26.3)\nRequirement already satisfied: Pillow in /usr/local/lib/python3.10/site-packages (from sentence_transformers) (10.2.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (3.13.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2023.12.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (4.9.0)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2.31.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (6.0.1)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (23.2)\nRequirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (11.0.2.54)\nRequirement already satisfied: nvidia-nccl-cu12==2.18.1 in /usr/local/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (2.18.1)\nRequirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (12.1.3.1)\nRequirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (11.4.5.107)\nRequirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (2.1.0)\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (1.12)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (12.1.105)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (12.1.105)\nRequirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (12.1.0.106)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (3.2.1)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (12.1.105)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (3.1.3)\nRequirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (12.1.105)\nRequirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (8.9.2.26)\nRequirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (10.3.2.106)\nRequirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence_transformers) (12.3.101)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/site-packages (from transformers<5.0.0,>=4.32.0->sentence_transformers) (2023.12.25)\nRequirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/site-packages (from transformers<5.0.0,>=4.32.0->sentence_transformers) (0.4.2)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/site-packages (from transformers<5.0.0,>=4.32.0->sentence_transformers) (0.15.1)\nRequirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/site-packages (from scikit-learn->sentence_transformers) (1.3.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/site-packages (from scikit-learn->sentence_transformers) (3.2.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence_transformers) (2.1.4)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (2023.11.17)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (2.1.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (3.6)\nRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/site-packages (from sympy->torch>=1.11.0->sentence_transformers) (1.3.0)\nInstalling collected packages: sentence_transformers\nSuccessfully installed sentence_transformers-2.6.1\n\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n\u001B[0m\n\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.0.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m24.0\u001B[0m\n\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\nCollecting chromadb\n  Downloading chromadb-0.4.24-py3-none-any.whl (525 kB)\n\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m525.5/525.5 kB\u001B[0m \u001B[31m3.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\n\u001B[?25hRequirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.10/site-packages (from chromadb) (3.10.0)\nCollecting posthog>=2.4.0\n  Downloading posthog-3.5.0-py2.py3-none-any.whl (41 kB)\n\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m41.3/41.3 kB\u001B[0m \u001B[31m3.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hRequirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.10/site-packages (from chromadb) (4.66.1)\nCollecting mmh3>=4.0.1\n  Downloading mmh3-4.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (67 kB)\n\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m67.6/67.6 kB\u001B[0m \u001B[31m5.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hCollecting onnxruntime>=1.14.1\n  Downloading onnxruntime-1.17.3-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.8 MB)\n\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m6.8/6.8 MB\u001B[0m \u001B[31m41.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\n\u001B[?25hCollecting opentelemetry-api>=1.2.0\n  Downloading opentelemetry_api-1.24.0-py3-none-any.whl (60 kB)\n\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m60.1/60.1 kB\u001B[0m \u001B[31m2.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hCollecting typer>=0.9.0\n  Downloading typer-0.12.3-py3-none-any.whl (47 kB)\n\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m47.2/47.2 kB\u001B[0m \u001B[31m4.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hCollecting kubernetes>=28.1.0\n  Downloading kubernetes-29.0.0-py2.py3-none-any.whl (1.6 MB)\n\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.6/1.6 MB\u001B[0m \u001B[31m61.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hRequirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.10/site-packages (from chromadb) (1.26.3)\nRequirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.10/site-packages (from chromadb) (2.7.0)\nRequirement already satisfied: PyYAML>=6.0.0 in /usr/local/lib/python3.10/site-packages (from chromadb) (6.0.1)\nCollecting opentelemetry-sdk>=1.2.0\n  Downloading opentelemetry_sdk-1.24.0-py3-none-any.whl (106 kB)\n\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m106.1/106.1 kB\u001B[0m \u001B[31m10.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hCollecting pypika>=0.48.9\n  Downloading PyPika-0.48.9.tar.gz (67 kB)\n\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m67.3/67.3 kB\u001B[0m \u001B[31m4.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25h  Installing build dependencies ... \u001B[?25ldone\n\u001B[?25h  Getting requirements to build wheel ... \u001B[?25ldone\n\u001B[?25h  Preparing metadata (pyproject.toml) ... \u001B[?25ldone\n\u001B[?25hCollecting bcrypt>=4.0.1\n  Downloading bcrypt-4.1.2-cp39-abi3-manylinux_2_28_x86_64.whl (698 kB)\n\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m698.9/698.9 kB\u001B[0m \u001B[31m39.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hCollecting chroma-hnswlib==0.7.3\n  Downloading chroma_hnswlib-0.7.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m2.4/2.4 MB\u001B[0m \u001B[31m61.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m:00:01\u001B[0m\n\u001B[?25hRequirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.10/site-packages (from chromadb) (7.6.0)\nRequirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/site-packages (from chromadb) (4.9.0)\nCollecting opentelemetry-instrumentation-fastapi>=0.41b0\n  Downloading opentelemetry_instrumentation_fastapi-0.45b0-py3-none-any.whl (11 kB)\nRequirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.10/site-packages (from chromadb) (1.60.0)\nCollecting uvicorn[standard]>=0.18.3\n  Downloading uvicorn-0.29.0-py3-none-any.whl (60 kB)\n\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m60.8/60.8 kB\u001B[0m \u001B[31m5.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hRequirement already satisfied: importlib-resources in /usr/local/lib/python3.10/site-packages (from chromadb) (6.1.1)\nRequirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.10/site-packages (from chromadb) (8.2.3)\nRequirement already satisfied: requests>=2.28 in /usr/local/lib/python3.10/site-packages (from chromadb) (2.31.0)\nCollecting pulsar-client>=3.1.0\n  Downloading pulsar_client-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m5.4/5.4 MB\u001B[0m \u001B[31m68.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m:00:01\u001B[0m00:01\u001B[0m\n\u001B[?25hCollecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0\n  Downloading opentelemetry_exporter_otlp_proto_grpc-1.24.0-py3-none-any.whl (18 kB)\nRequirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.10/site-packages (from chromadb) (0.15.1)\nCollecting fastapi>=0.95.2\n  Downloading fastapi-0.110.1-py3-none-any.whl (91 kB)\n\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m91.9/91.9 kB\u001B[0m \u001B[31m7.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hCollecting build>=1.0.3\n  Downloading build-1.2.1-py3-none-any.whl (21 kB)\nCollecting pyproject_hooks\n  Downloading pyproject_hooks-1.0.0-py3-none-any.whl (9.3 kB)\nRequirement already satisfied: packaging>=19.1 in /usr/local/lib/python3.10/site-packages (from build>=1.0.3->chromadb) (23.2)\nRequirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/site-packages (from build>=1.0.3->chromadb) (2.0.1)\nCollecting starlette<0.38.0,>=0.37.2\n  Downloading starlette-0.37.2-py3-none-any.whl (71 kB)\n\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m71.9/71.9 kB\u001B[0m \u001B[31m8.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hRequirement already satisfied: requests-oauthlib in /usr/local/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (1.3.1)\nRequirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (2023.11.17)\nRequirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (2.1.0)\nRequirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\nRequirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\nRequirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (2.26.2)\nRequirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (2.8.2)\nRequirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (1.7.0)\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (1.12)\nCollecting coloredlogs\n  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m46.0/46.0 kB\u001B[0m \u001B[31m4.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hRequirement already satisfied: flatbuffers in /usr/local/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (23.5.26)\nRequirement already satisfied: protobuf in /usr/local/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (3.20.3)\nCollecting deprecated>=1.2.6\n  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\nCollecting importlib-metadata<=7.0,>=6.0\n  Downloading importlib_metadata-7.0.0-py3-none-any.whl (23 kB)\nRequirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.62.0)\nCollecting opentelemetry-exporter-otlp-proto-common==1.24.0\n  Downloading opentelemetry_exporter_otlp_proto_common-1.24.0-py3-none-any.whl (17 kB)\nCollecting opentelemetry-proto==1.24.0\n  Downloading opentelemetry_proto-1.24.0-py3-none-any.whl (50 kB)\n\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m50.8/50.8 kB\u001B[0m \u001B[31m4.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hCollecting opentelemetry-instrumentation-asgi==0.45b0\n  Downloading opentelemetry_instrumentation_asgi-0.45b0-py3-none-any.whl (14 kB)\nCollecting opentelemetry-util-http==0.45b0\n  Downloading opentelemetry_util_http-0.45b0-py3-none-any.whl (6.9 kB)\nCollecting opentelemetry-semantic-conventions==0.45b0\n  Downloading opentelemetry_semantic_conventions-0.45b0-py3-none-any.whl (36 kB)\nCollecting opentelemetry-instrumentation==0.45b0\n  Downloading opentelemetry_instrumentation-0.45b0-py3-none-any.whl (28 kB)\nRequirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/site-packages (from opentelemetry-instrumentation==0.45b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.14.1)\nRequirement already satisfied: setuptools>=16.0 in /usr/local/lib/python3.10/site-packages (from opentelemetry-instrumentation==0.45b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (65.5.1)\nCollecting asgiref~=3.0\n  Downloading asgiref-3.8.1-py3-none-any.whl (23 kB)\nCollecting monotonic>=1.5\n  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\nCollecting backoff>=1.10.0\n  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\nRequirement already satisfied: pydantic-core==2.18.1 in /usr/local/lib/python3.10/site-packages (from pydantic>=1.9->chromadb) (2.18.1)\nRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/site-packages (from pydantic>=1.9->chromadb) (0.6.0)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests>=2.28->chromadb) (3.6)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/site-packages (from requests>=2.28->chromadb) (3.3.2)\nRequirement already satisfied: huggingface_hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/site-packages (from tokenizers>=0.13.2->chromadb) (0.20.3)\nRequirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/site-packages (from typer>=0.9.0->chromadb) (8.1.7)\nCollecting shellingham>=1.3.0\n  Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\nRequirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/site-packages (from typer>=0.9.0->chromadb) (13.7.0)\nRequirement already satisfied: h11>=0.8 in /usr/local/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.14.0)\nCollecting httptools>=0.5.0\n  Downloading httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (341 kB)\n\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m341.4/341.4 kB\u001B[0m \u001B[31m15.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hCollecting python-dotenv>=0.13\n  Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\nCollecting websockets>=10.4\n  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m130.2/130.2 kB\u001B[0m \u001B[31m7.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hCollecting watchfiles>=0.13\n  Downloading watchfiles-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.3/1.3 MB\u001B[0m \u001B[31m12.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m:00:01\u001B[0m\n\u001B[?25hCollecting uvloop!=0.15.0,!=0.15.1,>=0.14.0\n  Downloading uvloop-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m3.4/3.4 MB\u001B[0m \u001B[31m82.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m:00:01\u001B[0m\n\u001B[?25hRequirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.3.2)\nRequirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.3.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.13.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2023.12.2)\nRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/site-packages (from importlib-metadata<=7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.17.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/site-packages (from rich>=10.11.0->typer>=0.9.0->chromadb) (2.17.2)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/site-packages (from rich>=10.11.0->typer>=0.9.0->chromadb) (3.0.0)\nRequirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.10/site-packages (from starlette<0.38.0,>=0.37.2->fastapi>=0.95.2->chromadb) (4.2.0)\nCollecting humanfriendly>=9.1\n  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m86.8/86.8 kB\u001B[0m \u001B[31m10.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\nRequirement already satisfied: exceptiongroup>=1.0.2 in /usr/local/lib/python3.10/site-packages (from anyio<5,>=3.4.0->starlette<0.38.0,>=0.37.2->fastapi>=0.95.2->chromadb) (1.2.0)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/site-packages (from anyio<5,>=3.4.0->starlette<0.38.0,>=0.37.2->fastapi>=0.95.2->chromadb) (1.3.0)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer>=0.9.0->chromadb) (0.1.2)\nRequirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.5.1)\nBuilding wheels for collected packages: pypika\n  Building wheel for pypika (pyproject.toml) ... \u001B[?25ldone\n\u001B[?25h  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53724 sha256=1055f5f7ca1d4af934ccf02ab732340de31b9ef07d1eac0a475519069a0a33c2\n  Stored in directory: /root/.cache/pip/wheels/e1/26/51/d0bffb3d2fd82256676d7ad3003faea3bd6dddc9577af665f4\nSuccessfully built pypika\nInstalling collected packages: pypika, monotonic, mmh3, websockets, uvloop, uvicorn, shellingham, python-dotenv, pyproject_hooks, pulsar-client, opentelemetry-util-http, opentelemetry-semantic-conventions, opentelemetry-proto, importlib-metadata, humanfriendly, httptools, deprecated, chroma-hnswlib, bcrypt, backoff, asgiref, watchfiles, starlette, posthog, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, coloredlogs, build, typer, opentelemetry-sdk, opentelemetry-instrumentation, onnxruntime, kubernetes, fastapi, opentelemetry-instrumentation-asgi, opentelemetry-exporter-otlp-proto-grpc, opentelemetry-instrumentation-fastapi, chromadb\n  Attempting uninstall: importlib-metadata\n    Found existing installation: importlib-metadata 7.0.1\n    Uninstalling importlib-metadata-7.0.1:\n      Successfully uninstalled importlib-metadata-7.0.1\nSuccessfully installed asgiref-3.8.1 backoff-2.2.1 bcrypt-4.1.2 build-1.2.1 chroma-hnswlib-0.7.3 chromadb-0.4.24 coloredlogs-15.0.1 deprecated-1.2.14 fastapi-0.110.1 httptools-0.6.1 humanfriendly-10.0 importlib-metadata-7.0.0 kubernetes-29.0.0 mmh3-4.1.0 monotonic-1.6 onnxruntime-1.17.3 opentelemetry-api-1.24.0 opentelemetry-exporter-otlp-proto-common-1.24.0 opentelemetry-exporter-otlp-proto-grpc-1.24.0 opentelemetry-instrumentation-0.45b0 opentelemetry-instrumentation-asgi-0.45b0 opentelemetry-instrumentation-fastapi-0.45b0 opentelemetry-proto-1.24.0 opentelemetry-sdk-1.24.0 opentelemetry-semantic-conventions-0.45b0 opentelemetry-util-http-0.45b0 posthog-3.5.0 pulsar-client-3.5.0 pypika-0.48.9 pyproject_hooks-1.0.0 python-dotenv-1.0.1 shellingham-1.5.4 starlette-0.37.2 typer-0.12.3 uvicorn-0.29.0 uvloop-0.19.0 watchfiles-0.21.0 websockets-12.0\n\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n\u001B[0m\n\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.0.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m24.0\u001B[0m\n\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Select a backend\n",
    "Keras is a high-level, multi-framework deep learning API designed for simplicity and ease of use. Using Keras 3, you can run workflows on one of three backends: TensorFlow, JAX, or PyTorch."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"jax\"  # Or \"torch\" or \"tensorflow\".\n",
    "# Avoid memory fragmentation on JAX backend.\n",
    "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"]=\"1.00\"\n",
    "\n",
    "# Import Keras and KerasNLP.\n",
    "import keras\n",
    "import keras_nlp"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-13T22:02:44.190810Z",
     "iopub.execute_input": "2024-04-13T22:02:44.191100Z",
     "iopub.status.idle": "2024-04-13T22:02:56.746417Z",
     "shell.execute_reply.started": "2024-04-13T22:02:44.191058Z",
     "shell.execute_reply": "2024-04-13T22:02:56.745709Z"
    },
    "trusted": true
   },
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "text": "/usr/local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Loading the Model\n",
    "\n",
    "KerasNLP provides implementations of many popular model architectures.We will create a model using GemmaCausalLM, an end-to-end Gemma model for causal language modeling. A causal language model predicts the next token based on previous tokens."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Loading Instruct Gemma_2b\n",
    "import gc\n",
    "gc.collect()\n",
    "gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(\"gemma_instruct_2b_en\")\n",
    "gemma_lm.summary()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-13T22:02:56.747272Z",
     "iopub.execute_input": "2024-04-13T22:02:56.747662Z",
     "iopub.status.idle": "2024-04-13T22:03:55.963768Z",
     "shell.execute_reply.started": "2024-04-13T22:02:56.747635Z",
     "shell.execute_reply": "2024-04-13T22:03:55.962864Z"
    },
    "trusted": true
   },
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "text": "Attaching 'config.json' from model 'keras/gemma/keras/gemma_instruct_2b_en/2' to your Kaggle notebook...\nAttaching 'config.json' from model 'keras/gemma/keras/gemma_instruct_2b_en/2' to your Kaggle notebook...\nAttaching 'model.weights.h5' from model 'keras/gemma/keras/gemma_instruct_2b_en/2' to your Kaggle notebook...\nAttaching 'tokenizer.json' from model 'keras/gemma/keras/gemma_instruct_2b_en/2' to your Kaggle notebook...\nAttaching 'assets/tokenizer/vocabulary.spm' from model 'keras/gemma/keras/gemma_instruct_2b_en/2' to your Kaggle notebook...\nnormalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "\u001B[1mPreprocessor: \"gemma_causal_lm_preprocessor\"\u001B[0m\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"gemma_causal_lm_preprocessor\"</span>\n</pre>\n"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃\u001B[1m \u001B[0m\u001B[1mTokenizer (type)                                  \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1m                                            Vocab #\u001B[0m\u001B[1m \u001B[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ gemma_tokenizer (\u001B[38;5;33mGemmaTokenizer\u001B[0m)                   │                                             \u001B[38;5;34m256,000\u001B[0m │\n└────────────────────────────────────────────────────┴─────────────────────────────────────────────────────┘\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Tokenizer (type)                                   </span>┃<span style=\"font-weight: bold\">                                             Vocab # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ gemma_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaTokenizer</span>)                   │                                             <span style=\"color: #00af00; text-decoration-color: #00af00\">256,000</span> │\n└────────────────────────────────────────────────────┴─────────────────────────────────────────────────────┘\n</pre>\n"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "\u001B[1mModel: \"gemma_causal_lm\"\u001B[0m\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gemma_causal_lm\"</span>\n</pre>\n"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃\u001B[1m \u001B[0m\u001B[1mLayer (type)                 \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mOutput Shape             \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1m        Param #\u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mConnected to              \u001B[0m\u001B[1m \u001B[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ padding_mask (\u001B[38;5;33mInputLayer\u001B[0m)     │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;45mNone\u001B[0m)              │               \u001B[38;5;34m0\u001B[0m │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_ids (\u001B[38;5;33mInputLayer\u001B[0m)        │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;45mNone\u001B[0m)              │               \u001B[38;5;34m0\u001B[0m │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ gemma_backbone                │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m2048\u001B[0m)        │   \u001B[38;5;34m2,506,172,416\u001B[0m │ padding_mask[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m],        │\n│ (\u001B[38;5;33mGemmaBackbone\u001B[0m)               │                           │                 │ token_ids[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m]            │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_embedding               │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m256000\u001B[0m)      │     \u001B[38;5;34m524,288,000\u001B[0m │ gemma_backbone[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m]       │\n│ (\u001B[38;5;33mReversibleEmbedding\u001B[0m)         │                           │                 │                            │\n└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ gemma_backbone                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)        │   <span style=\"color: #00af00; text-decoration-color: #00af00\">2,506,172,416</span> │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaBackbone</span>)               │                           │                 │ token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_embedding               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256000</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">524,288,000</span> │ gemma_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbedding</span>)         │                           │                 │                            │\n└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n</pre>\n"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "\u001B[1m Total params: \u001B[0m\u001B[38;5;34m2,506,172,416\u001B[0m (9.34 GB)\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,506,172,416</span> (9.34 GB)\n</pre>\n"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "\u001B[1m Trainable params: \u001B[0m\u001B[38;5;34m2,506,172,416\u001B[0m (9.34 GB)\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,506,172,416</span> (9.34 GB)\n</pre>\n"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "\u001B[1m Non-trainable params: \u001B[0m\u001B[38;5;34m0\u001B[0m (0.00 B)\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prepare fine-tune data\n",
    "\n",
    "Alternatively, skipping this part just directly reading /kaggle/input/data-science/data_science.txt\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "RETRAIN = False"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-13T22:03:55.965930Z",
     "iopub.execute_input": "2024-04-13T22:03:55.966272Z",
     "iopub.status.idle": "2024-04-13T22:03:55.969915Z",
     "shell.execute_reply.started": "2024-04-13T22:03:55.966239Z",
     "shell.execute_reply": "2024-04-13T22:03:55.969177Z"
    },
    "trusted": true
   },
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "if RETRAIN:\n",
    "    !pip install cryptography>=3.1\n",
    "    import os\n",
    "    from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "    from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "    from tqdm import tqdm"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-13T22:03:55.970872Z",
     "iopub.execute_input": "2024-04-13T22:03:55.971147Z",
     "iopub.status.idle": "2024-04-13T22:03:55.985219Z",
     "shell.execute_reply.started": "2024-04-13T22:03:55.971121Z",
     "shell.execute_reply": "2024-04-13T22:03:55.984392Z"
    },
    "trusted": true
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "There are numbers text splitter to try in [LangChain Texty Splitters](https://python.langchain.com/docs/modules/data_connection/document_transformers/)\n",
    "Here we tried with RecursiveCharacterTextSplitter for effiency. It also has the effect of trying to keep all paragraphs (and then sentences, and then words) together as long as possible, as those would generically seem to be the strongest semantically related pieces of text."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "gc.collect()\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-13T22:03:55.986220Z",
     "iopub.execute_input": "2024-04-13T22:03:55.986490Z",
     "iopub.status.idle": "2024-04-13T22:03:56.397804Z",
     "shell.execute_reply.started": "2024-04-13T22:03:55.986466Z",
     "shell.execute_reply": "2024-04-13T22:03:56.396612Z"
    },
    "trusted": true
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "if RETRAIN:\n",
    "    # Prepare fine tune data\n",
    "    def get_all_pdfs(directory):\n",
    "        \"\"\"Get the list of pdf files in the directory.\"\"\"\n",
    "        pdf_files = []\n",
    "        for root, dirs, files in os.walk(directory):\n",
    "            for file in files:\n",
    "                if file.endswith(\".pdf\"):\n",
    "                    pdf_files.append(os.path.join(root, file))\n",
    "        return pdf_files\n",
    "\n",
    "    # load documents\n",
    "    pdf_files = get_all_pdfs('/kaggle/input/data-science-cheat-sheets')\n",
    "    loaders = [PyPDFLoader(pdf_file) for pdf_file in pdf_files]\n",
    "    all_documents = []\n",
    "    for loader in tqdm(loaders):\n",
    "        raw_documents = loader.load_and_split()\n",
    "        # split the documents into smaller chunks\n",
    "        documents = text_splitter.split_documents(raw_documents)\n",
    "        all_documents.extend(documents)\n",
    "\n",
    "    # write them down.\n",
    "    with open(\"/kaggle/input/data-science/data_science.txt\", \"w\") as f:\n",
    "        for d in all_domuments:\n",
    "            f.write(d.page_content)\n",
    "    "
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-13T22:03:56.398893Z",
     "iopub.execute_input": "2024-04-13T22:03:56.399192Z",
     "iopub.status.idle": "2024-04-13T22:03:56.405864Z",
     "shell.execute_reply.started": "2024-04-13T22:03:56.399164Z",
     "shell.execute_reply": "2024-04-13T22:03:56.404972Z"
    },
    "trusted": true
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Inference\n",
    "To help with accuracy, we adopt 3 strageties here.\n",
    "1. RAG with wikipedia data.\n",
    "2. Fine tune with data science cheat sheet\n",
    "3. Chain-of-thoughts prompting.\n",
    "\n",
    "\n",
    "For #1, we are going to use WikipediaRetriever to retrieve wiki pages from wikipedia.org into the Document format. This additional infomation will be used to providing extra context to gemma model to generate response. \n",
    "\n",
    "For #2, we are going to use LoRA to fine tune gemma model with domain specific knowledge.\n",
    "\n",
    "For #3, the idea is first process the query, extracting terms related to data science, let the LLM first explain those terms before answering the question directly.\n",
    "\n",
    "**The quality of tuning data is crutial. Low quality training data does more harm than good!**\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Set 1:** Load data from data science cheat sheets.\n",
    "This dataset needs cleaning before being used for fine tuning."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "with open(\"/kaggle/input/data-science/data_science.txt\") as f:\n",
    "    data_science = f.read()\n",
    "texts = text_splitter.create_documents([data_science])\n",
    "data = [t.page_content for t in texts]"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-13T22:03:56.406919Z",
     "iopub.execute_input": "2024-04-13T22:03:56.407164Z",
     "iopub.status.idle": "2024-04-13T22:03:56.906581Z",
     "shell.execute_reply.started": "2024-04-13T22:03:56.407141Z",
     "shell.execute_reply": "2024-04-13T22:03:56.905408Z"
    },
    "trusted": true
   },
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Set 2:** Load data from kaggle-docs"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(f\"/kaggle/input/kaggle-docs/questions_answers/data.csv\")"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-13T22:03:56.907809Z",
     "iopub.execute_input": "2024-04-13T22:03:56.908150Z",
     "iopub.status.idle": "2024-04-13T22:03:56.942970Z",
     "shell.execute_reply.started": "2024-04-13T22:03:56.908121Z",
     "shell.execute_reply": "2024-04-13T22:03:56.942150Z"
    },
    "trusted": true
   },
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "template = \"Question:\\n{Question}\\n\\nAnswer:\\n{Answer}\"\n",
    "df[\"prompt\"] = df.apply(lambda row: template.format(Question=row.Question,\n",
    "                                                             Answer=row.Answer), axis=1)\n",
    "kaggle_data = df.prompt.tolist()\n",
    "kaggle_data[:3]"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-13T22:03:56.945979Z",
     "iopub.execute_input": "2024-04-13T22:03:56.946368Z",
     "iopub.status.idle": "2024-04-13T22:03:56.955456Z",
     "shell.execute_reply.started": "2024-04-13T22:03:56.946327Z",
     "shell.execute_reply": "2024-04-13T22:03:56.954577Z"
    },
    "trusted": true
   },
   "execution_count": 10,
   "outputs": [
    {
     "execution_count": 10,
     "output_type": "execute_result",
     "data": {
      "text/plain": "['Question:\\nWhat are the different types of competitions available on Kaggle?\\n\\nAnswer:\\n# Types of Competitions\\n\\nKaggle Competitions are designed to provide challenges for competitors at all different stages of their machine learning careers. As a result, they are very diverse, with a range of broad types.\\n\\n## Featured\\n\\nFeatured competitions are the types of competitions that Kaggle is probably best known for. These are full-scale machine learning challenges which pose difficult, generally commercially-purposed prediction problems. For example, past featured competitions have included:\\n\\n- [Allstate Claim Prediction Challenge](https://www.kaggle.com/c/allstate-purchase-prediction-challenge) - Use customers’ shopping history to predict which insurance policy they purchase\\n- [Jigsaw Toxic Comment Classification Challenge](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge) - Predict the existence and type of toxic comments on Wikipedia\\n- [Zillow Prize](https://www.kaggle.com/c/zillow-prize-1) - Build a machine learning algorithm that can challenge Zestimates, the Zillow real estate price estimation algorithm\\n\\nFeatured competitions attract some of the most formidable experts, and offer prize pools going as high as a million dollars. However, they remain accessible to anyone and everyone. Whether you’re an expert in the field or a complete novice, featured competitions are a valuable opportunity to learn skills and techniques from the very best in the field.\\n\\n## Research\\n\\nResearch competitions are another common type of competition on Kaggle. Research competitions feature problems which are more experimental than featured competition problems. For example, some past research competitions have included:\\n\\n- [Google Landmark Retrieval Challenge](https://www.kaggle.com/c/landmark-retrieval-challenge) - Given an image, can you find all the same landmarks in a dataset?\\n- [Right Whale Recognition](https://www.kaggle.com/c/noaa-right-whale-recognition) - Identify endangered right whales in aerial photographs\\n- [Large Scale Hierarchical Text Classification](https://www.kaggle.com/c/lshtc) - Classify Wikipedia documents into one of ~300,000 categories\\n\\nResearch competitions do not usually offer prizes or points due to their experimental nature. But they offer an opportunity to work on problems which may not have a clean or easy solution and which are integral to a specific domain or area in a slightly less competitive environment.\\n\\n## Getting Started\\n\\nGetting Started competitions are the easiest, most approachable competitions on Kaggle. These are semi-permanent competitions that are meant to be used by new users just getting their foot in the door in the field of machine learning. They offer no prizes or points. Because of their long-running nature, Getting Started competitions are perhaps the most heavily tutorialized problems in machine learning - just what a newcomer needs to get started!\\n\\n- [Digit Recognizer](https://www.kaggle.com/c/digit-recognizer)\\n- [Titanic: Machine Learning from Disaster](https://www.kaggle.com/c/titanic) - Predict survival on the Titanic\\n- [Housing Prices: Advanced Regression Techniques](https://www.kaggle.com/c/house-prices-advanced-regression-techniques)\\n\\nGetting Started competitions have two-month rolling leaderboards. Once a submission is more than two months old, it is automatically invalidated and no longer counts towards the leaderboard. Similarly, your team will drop from the leaderboard if all its submissions are older than two months. This gives new Kagglers the opportunity to see how their scores stack up against a cohort of competitors, rather than many tens of thousands of users. If your team is removed from a Getting Started competition due to the rolling expiry and wishes to rejoin, creating a new submission will cause it to show again on the leaderboard.\\n\\nAdditionally, the Kaggle [Learn platform](https://www.kaggle.com/learn/overview) has several tracks for beginners interested in free hands-on data science learning from pandas to deep learning. Lessons within a track are separated into easily digestible chunks and contain Notebook exercises for you to practise building models and new techniques. You’ll learn all the skills you need to dive into Kaggle Competitions.\\n\\n## Playground\\n\\nPlayground competitions are a “for fun” type of Kaggle competition that is one step above Getting Started in difficulty. These are competitions which often provide relatively simple machine learning tasks, and are similarly targeted at newcomers or Kagglers interested in practicing a new type of problem in a lower-stakes setting. Prizes range from kudos to small cash prizes. Some examples of Playground competitions are:\\n\\n- [Dogs versus Cats](https://www.kaggle.com/c/dogs-vs-cats) - Create an algorithm to distinguish dogs from cats\\n- [Leaf Classification](https://www.kaggle.com/c/leaf-classification) - Can you see the random forest for the leaves?\\n- [New York City Taxi Trip Duration](https://www.kaggle.com/c/nyc-taxi-trip-duration) - Share code and data to improve ride time predictions',\n 'Question:\\nWhat are the different competition formats on Kaggle?\\n\\nAnswer:\\nThere are handful of different formats competitions are run in.\\n\\n## Simple Competitions\\n\\nSimple (or “classic”) competitions are those which follow the standard Kaggle format. In a simple competition, users can access the complete datasets at the beginning of the competition, after accepting the competition’s rules. As a competitor you will download the data, build models on it locally or in [Notebooks](https://www.kaggle.com/notebooks), generate a prediction file, then upload your predictions as a submission on Kaggle. By far most competitions on Kaggle follow this format.\\n\\nOne example of a simple competition is the Porto Seguro Safe Driver Prediction Competition [Porto Seguro Safe Driver Prediction Competition](https://www.kaggle.com/c/porto-seguro-safe-driver-prediction).\\n\\n## Two-stage Competitions\\n\\nIn two-stage competitions the challenge is split into two parts: Stage 1 and Stage 2, with the second stage building on the results teams achieved in Stage 1. Stage 2 involves a new test dataset that is released at the start of the stage. Eligibility for Stage 2 typically requires making a submission in Stage 1. In two-stage competitions, it’s especially important to read and understand the competition’s specific rules and timeline.\\n\\nOne example of such a competition is the Nature Conservancy Fisheries Monitoring Competition [Nature Conservancy Fisheries Monitoring Competition](https://www.kaggle.com/c/the-nature-conservancy-fisheries-monitoring).\\n\\n## Code Competitions\\n\\nSome competitions are code competitions. In these competitions all submissions are made from inside of a Kaggle Notebook, and it is not possible to upload submissions to the Competition directly.\\n\\nThese competitions have two attractive features. The competition is more balanced, as all users have the same hardware allowances. And the winning models tend to be far simpler than the winning models in other competitions, as they must be made to run within the compute constraints imposed by the platform.\\n\\nCode competitions are configured with their own unique constraints on the Notebooks you can submit. These may be restricted by characteristics like: CPU or GPU runtime, ability to use external data, and access to the internet. To learn the constraints you must adhere to, review the Requirements for that specific competition.\\n\\nAn example of a code competition is Quora Insincere Questions Classification [Quora Insincere Questions Classification](https://www.kaggle.com/c/quora-insincere-questions-classification).\\n\\n### Code Competition FAQ\\n\\n**I\\'m getting errors when submitting. What should I do?**\\n\\n1. Please see our page on code competition debugging [code competition debugging](https://www.kaggle.com/code-competition-debugging) for tips on understanding and preventing submission errors.\\n\\n2. First you\\'ll need to write a Notebook which reads the Competition\\'s dataset and makes predictions on the test set. Specifically, have your Notebook write your predictions to a \"submission file\", which is typically a submission.csv file, though some competitions have special formats. See the competition\\'s Evaluation page, or look for sample_submission.csv (or similar) in the Data page for more information on the expected name and format of your submission file.\\n\\n3. Save a full version of your Notebook by clicking \"Save Version\" and selecting \"Save & Run All\". This saves your code, runs it, and creates a version of the code and output. Once your save finishes, navigate to the Viewer page for your new Notebook Version.\\n\\n4. In the Notebook Viewer, navigate to the Output section, find and select the submission file you created, and click the \"Submit\" button.\\n\\n**Can I upload external data?**\\n\\nSome competitions allow external data and some do not. If a competition allows external data, you can attach it to your Notebook by adding it as a data source. If a competition does not allow external data, attaching it to your Notebook will deactivate the \"Submit\" button on the associated saved version.\\n\\n**What are the compute limits of Notebooks?**\\n\\nThe compute limits of the Notebooks workers are subject to change. You can view the site-wide memory, CPU, runtime limits, and other limits from the editor.\\n\\nCode competitions come in many shapes and sizes, and will often impose limits specific to a competition. You should view the competition description to understand if these limits are activated and what they are. Example variations include:\\n\\n- Specific runtime limits\\n- Specific limits that apply to Notebooks using GPUs\\n- Internet access allowed or disallowed\\n- External data allowed or disallowed\\n- Custom package installs allowed or disallowed\\n- Submission file naming expectations\\n\\n**How do I team up in a code competition?**\\n\\nAll the competitions setup is the same as normal competitions, except that submissions are only made through Notebooks. To team up, go to the \"Team\" tab and invite others.\\n\\n**How will winners be determined?**\\n\\nIn some code competitions, winners will be determined by re-running selected submissions’ associated Notebooks on a private test set.\\n\\nIn such competitions, you will create your models in Notebooks and make submissions based on the test set provided on the Data page. You will make submissions from your Notebook using the above steps and select submissions for final judging from the “My Submissions” page, in the same manner as a regular competition.\\n\\nFollowing the competition deadline, your code will be rerun by Kaggle on a private test set that is not provided to you. Your model\\'s score against this private test set will determine your ranking on the private leaderboard and final standing in the competition.',\n 'Question:\\nHow to join a competition?\\n\\nAnswer:\\nBefore you start, navigate to the [Competitions listing](https://www.kaggle.com/competitions). It lists all of the currently active competitions.\\n\\nPublic competitions are viewable on Kaggle and appear in Kaggle search results. Depending on the privacy and access set by the host, some competitions may be unavailable for you to see or join. If a host set a competition\\'s visibility to private, you would only see the competition\\'s details if they shared a unique URL with you.\\n\\nIf you click on a specific Competition in the listing, you will go to the Competition’s homepage.\\n\\nThe first element worth calling out is the **Rules tab**. This contains the rules that govern your participation in the sponsor’s competition. You must accept the competition’s rules before downloading the data or making any submissions. It’s extremely important to read the rules before you start. This is doubly true if you are a new user. Users who do not abide by the rules may have their submissions invalidated at the end of the competition or banned from the platform. So please make sure to read and understand the rules before choosing to participate.\\n\\nIf anything is unclear or you have a question about participating, the competition’s forums are the perfect place to ask.\\n\\nThe information provided in the **Overview tabs** will vary from Competition to Competition. Five elements which are almost always included and should be reviewed are the “Description,” “Data”, “Evaluation,” “Timeline,” & “Prizes” sections.\\n\\n- The **description** gives an introduction into the competition’s objective and the sponsor’s goal in hosting it.\\n- The **data** tab is where you can download and learn more about the data used in the competition. You’ll use a training set to train models and a test set for which you’ll need to make your predictions. In most cases, the data or a subset of it is also accessible in Notebooks.\\n- The **evaluation** section describes how to format your submission file and how your submissions will be evaluated. Each competition employs a metric that serves as the objective measure for how competitors are ranked on the leaderboard.\\n- The **timeline** has detailed information on the competition timeline. Most Kaggle Competitions include, at a minimum, two deadlines: a rules acceptance deadline (after which point no new teams can join or merge in the competition), and a submission deadline (after which no new submissions will be accepted). It is very, very important to keep these deadlines in mind.\\n- The **prizes** section provides a breakdown of what prizes will be awarded to the winners, if prizes are relevant. This may come in the form of monetary, swag, or other perks. In addition to prizes, competitions may also award ranking points towards the Kaggle progression system. This is shown on the Overview page.\\n\\nReady to join? If the competition allows anyone to join, you should be able to click \"Join\" and accept the competition\\'s rules. If the competition has restricted access, the host will share a private link with you that allows you to join.\\n\\nOnce you have chosen a competition, read and accepted the rules, and made yourself aware of the competition deadlines, you are ready to submit!']"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Set 3:** DataScience_QA"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "qa = pd.read_csv(\"test12/main_data.csv\")\n",
    "qa[\"prompt\"] = qa.apply(lambda row: template.format(Question=row.question,Answer=row.answer), axis=1)                                                   \n",
    "qa_data = qa.prompt.tolist()\n",
    "qa_data[:3]"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-13T22:03:56.956498Z",
     "iopub.execute_input": "2024-04-13T22:03:56.956762Z",
     "iopub.status.idle": "2024-04-13T22:03:56.991718Z",
     "shell.execute_reply.started": "2024-04-13T22:03:56.956736Z",
     "shell.execute_reply": "2024-04-13T22:03:56.990992Z"
    },
    "trusted": true
   },
   "execution_count": 11,
   "outputs": [
    {
     "execution_count": 11,
     "output_type": "execute_result",
     "data": {
      "text/plain": "['Question:\\nWhat is Data Science?\\n\\nAnswer:\\nData science is an interdisciplinary field that uses scientific methods, processes, algorithms, and systems to extract knowledge and insights from structured and unstructured data.',\n 'Question:\\nWhat are the Key Components of Data Science?\\n\\nAnswer:\\nThe key components of data science include data collection, data cleaning and preprocessing, data analysis and modeling, interpretation of results, and communication of findings.',\n 'Question:\\nWhat is Data Collection?\\n\\nAnswer:\\nData collection involves gathering relevant data from various sources, such as databases, files, sensors, and web scraping, to address specific questions or objectives.']"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "LLMs are extremely large in size (parameters in the order of billions). Full fine-tuning (which updates all the parameters in the model) is not required for most applications because typical fine-tuning datasets are relatively much smaller than the pre-training datasets.\n",
    "\n",
    "Low Rank Adaptation (LoRA) is a fine-tuning technique which greatly reduces the number of trainable parameters for downstream tasks by freezing the weights of the model and inserting a smaller number of new weights into the model. This makes training with LoRA much faster and more memory-efficient, and produces smaller model weights (a few hundred MBs), all while maintaining the quality of the model outputs."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# finetune with data_science text.\n",
    "# Enable LoRA for the model and set the LoRA rank to 4.\n",
    "gemma_lm.backbone.enable_lora(rank=4)\n",
    "gemma_lm.summary()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-13T22:03:56.992576Z",
     "iopub.execute_input": "2024-04-13T22:03:56.992804Z",
     "iopub.status.idle": "2024-04-13T22:03:57.591889Z",
     "shell.execute_reply.started": "2024-04-13T22:03:56.992781Z",
     "shell.execute_reply": "2024-04-13T22:03:57.590936Z"
    },
    "trusted": true
   },
   "execution_count": 12,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "\u001B[1mPreprocessor: \"gemma_causal_lm_preprocessor\"\u001B[0m\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"gemma_causal_lm_preprocessor\"</span>\n</pre>\n"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃\u001B[1m \u001B[0m\u001B[1mTokenizer (type)                                  \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1m                                            Vocab #\u001B[0m\u001B[1m \u001B[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ gemma_tokenizer (\u001B[38;5;33mGemmaTokenizer\u001B[0m)                   │                                             \u001B[38;5;34m256,000\u001B[0m │\n└────────────────────────────────────────────────────┴─────────────────────────────────────────────────────┘\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Tokenizer (type)                                   </span>┃<span style=\"font-weight: bold\">                                             Vocab # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ gemma_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaTokenizer</span>)                   │                                             <span style=\"color: #00af00; text-decoration-color: #00af00\">256,000</span> │\n└────────────────────────────────────────────────────┴─────────────────────────────────────────────────────┘\n</pre>\n"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "\u001B[1mModel: \"gemma_causal_lm\"\u001B[0m\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gemma_causal_lm\"</span>\n</pre>\n"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃\u001B[1m \u001B[0m\u001B[1mLayer (type)                 \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mOutput Shape             \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1m        Param #\u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mConnected to              \u001B[0m\u001B[1m \u001B[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ padding_mask (\u001B[38;5;33mInputLayer\u001B[0m)     │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;45mNone\u001B[0m)              │               \u001B[38;5;34m0\u001B[0m │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_ids (\u001B[38;5;33mInputLayer\u001B[0m)        │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;45mNone\u001B[0m)              │               \u001B[38;5;34m0\u001B[0m │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ gemma_backbone                │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m2048\u001B[0m)        │   \u001B[38;5;34m2,507,536,384\u001B[0m │ padding_mask[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m],        │\n│ (\u001B[38;5;33mGemmaBackbone\u001B[0m)               │                           │                 │ token_ids[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m]            │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_embedding               │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m256000\u001B[0m)      │     \u001B[38;5;34m524,288,000\u001B[0m │ gemma_backbone[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m]       │\n│ (\u001B[38;5;33mReversibleEmbedding\u001B[0m)         │                           │                 │                            │\n└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ gemma_backbone                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)        │   <span style=\"color: #00af00; text-decoration-color: #00af00\">2,507,536,384</span> │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaBackbone</span>)               │                           │                 │ token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n│ token_embedding               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256000</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">524,288,000</span> │ gemma_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbedding</span>)         │                           │                 │                            │\n└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n</pre>\n"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "\u001B[1m Total params: \u001B[0m\u001B[38;5;34m2,507,536,384\u001B[0m (9.34 GB)\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,507,536,384</span> (9.34 GB)\n</pre>\n"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "\u001B[1m Trainable params: \u001B[0m\u001B[38;5;34m1,363,968\u001B[0m (5.20 MB)\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,363,968</span> (5.20 MB)\n</pre>\n"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "\u001B[1m Non-trainable params: \u001B[0m\u001B[38;5;34m2,506,172,416\u001B[0m (9.34 GB)\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,506,172,416</span> (9.34 GB)\n</pre>\n"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "FINE_TUNE = True"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-13T22:03:57.592978Z",
     "iopub.execute_input": "2024-04-13T22:03:57.593344Z",
     "iopub.status.idle": "2024-04-13T22:03:57.597104Z",
     "shell.execute_reply.started": "2024-04-13T22:03:57.593314Z",
     "shell.execute_reply": "2024-04-13T22:03:57.596301Z"
    },
    "trusted": true
   },
   "execution_count": 13,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "if FINE_TUNE:\n",
    "    # Limit the input sequence length to 512 (to control memory usage).\n",
    "    gemma_lm.preprocessor.sequence_length = 512\n",
    "    # Use AdamW (a common optimizer for transformer models).\n",
    "    optimizer = keras.optimizers.AdamW(\n",
    "        learning_rate=5e-5,\n",
    "        weight_decay=0.01,\n",
    "    )\n",
    "    # Exclude layernorm and bias terms from decay.\n",
    "    optimizer.exclude_from_weight_decay(var_names=[\"bias\", \"scale\"])\n",
    "\n",
    "    gemma_lm.compile(\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        optimizer=optimizer,\n",
    "        weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    "    )\n",
    "    #gemma_lm.fit(data, epochs=1, batch_size=1)\n",
    "    \n",
    "    # train on data from kaggle doc\n",
    "    gemma_lm.fit(kaggle_data, epochs=3, batch_size=1)\n",
    "    \n",
    "    # It is better to have a seperated llm just for assistent work, like query cleasing. But due to memory issue, only one llm instance can be created. \n",
    "    # assistent_lm = keras.saving.load_model(MODEL_PATH)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-13T22:03:57.598086Z",
     "iopub.execute_input": "2024-04-13T22:03:57.598337Z",
     "iopub.status.idle": "2024-04-13T22:04:52.038071Z",
     "shell.execute_reply.started": "2024-04-13T22:03:57.598313Z",
     "shell.execute_reply": "2024-04-13T22:04:52.037023Z"
    },
    "trusted": true
   },
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "text": "Epoch 1/3\n\u001B[1m60/60\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m34s\u001B[0m 153ms/step - loss: 2.3653 - sparse_categorical_accuracy: 0.4726\nEpoch 2/3\n\u001B[1m60/60\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m9s\u001B[0m 150ms/step - loss: 2.1124 - sparse_categorical_accuracy: 0.4805\nEpoch 3/3\n\u001B[1m60/60\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m9s\u001B[0m 150ms/step - loss: 1.9953 - sparse_categorical_accuracy: 0.4886\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install wikipedia"
   ],
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.status.busy": "2024-04-13T22:04:52.040356Z",
     "iopub.execute_input": "2024-04-13T22:04:52.040674Z",
     "iopub.status.idle": "2024-04-13T22:04:57.675335Z",
     "shell.execute_reply.started": "2024-04-13T22:04:52.040642Z",
     "shell.execute_reply": "2024-04-13T22:04:57.674114Z"
    },
    "trusted": true
   },
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "text": "Collecting wikipedia\n  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n  Preparing metadata (setup.py) ... \u001B[?25ldone\n\u001B[?25hRequirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/site-packages (from wikipedia) (4.12.3)\nRequirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/site-packages (from wikipedia) (2.31.0)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.1.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.6)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2023.11.17)\nRequirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/site-packages (from beautifulsoup4->wikipedia) (2.5)\nBuilding wheels for collected packages: wikipedia\n  Building wheel for wikipedia (setup.py) ... \u001B[?25ldone\n\u001B[?25h  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11678 sha256=916223d89ae41045e198531965e6b61a5f02662630f7d5a8b74f87bcd86f6169\n  Stored in directory: /root/.cache/pip/wheels/5e/b6/c5/93f3dec388ae76edc830cb42901bb0232504dfc0df02fc50de\nSuccessfully built wikipedia\nInstalling collected packages: wikipedia\nSuccessfully installed wikipedia-1.4.0\n\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n\u001B[0m\n\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.0.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m24.0\u001B[0m\n\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from langchain_community.retrievers import WikipediaRetriever\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "retriever = WikipediaRetriever(top_k_results=2)\n",
    "template = \"\"\"You are a data scientist that answer questions in data science domain in an easy to understand and eloquent way. Use the given Context as reference if it is relevant to the question.\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer: \n",
    "\"\"\""
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-13T22:04:57.676976Z",
     "iopub.execute_input": "2024-04-13T22:04:57.677320Z",
     "iopub.status.idle": "2024-04-13T22:04:58.193673Z",
     "shell.execute_reply.started": "2024-04-13T22:04:57.677287Z",
     "shell.execute_reply": "2024-04-13T22:04:58.192484Z"
    },
    "trusted": true
   },
   "execution_count": 16,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import string\n",
    "string.punctuation"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-13T22:04:58.194823Z",
     "iopub.execute_input": "2024-04-13T22:04:58.195829Z",
     "iopub.status.idle": "2024-04-13T22:04:58.201115Z",
     "shell.execute_reply.started": "2024-04-13T22:04:58.195794Z",
     "shell.execute_reply": "2024-04-13T22:04:58.200272Z"
    },
    "trusted": true
   },
   "execution_count": 17,
   "outputs": [
    {
     "execution_count": 17,
     "output_type": "execute_result",
     "data": {
      "text/plain": "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "TO_CLEAN = set([w for w in ENGLISH_STOP_WORDS])\n",
    "for c in string.punctuation:\n",
    "    TO_CLEAN.add(c)\n",
    "TO_CLEAN.add('does')"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-13T22:04:58.202185Z",
     "iopub.execute_input": "2024-04-13T22:04:58.202429Z",
     "iopub.status.idle": "2024-04-13T22:04:58.213347Z",
     "shell.execute_reply.started": "2024-04-13T22:04:58.202405Z",
     "shell.execute_reply": "2024-04-13T22:04:58.212594Z"
    },
    "trusted": true
   },
   "execution_count": 18,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def clean_query(query):\n",
    "    q = query.strip()\n",
    "    new_text = \" \".join([w for w in q.split() if w.lower() not in TO_CLEAN])\n",
    "    return new_text\n",
    "\n",
    "# few-shot prompting to extract keywords and explain them.\n",
    "def extract_key_words(query):\n",
    "    prompt = \"\"\" In the following text, identify terms that are relevant to Data Science and explain the terms. Follow the examples for formatting. \n",
    "    \n",
    "    Text: Matplot is a python library that help us to plot data. The easiest and most basic plots are line, scatter and histogram plots.\n",
    "    Answer: \n",
    "     Matplot: Matplot in Matplotlib is a Python library that lets you create all sorts of plots, charts, and graphs. Think of it as your digital art kit for visual storytelling with data.\n",
    "     histogram plots: A histogram plot is a visualization tool used to understand the distribution of numerical data. \n",
    "     \n",
    "    Text: Pandas Python library\n",
    "    Answer: \n",
    "     Pandas: Pandas is built on top of the Python programming language and designed specifically to make working with structured, tabular data both easy and intuitive.\n",
    "   \n",
    "    Text: {text}\n",
    "    Answer:\n",
    "    \"\"\"\n",
    "    p = prompt.format(\n",
    "        text=clean_query(query),\n",
    "    )\n",
    "\n",
    "    result = gemma_lm.generate(p)\n",
    "    idx = result.rfind(\"Answer:\")\n",
    "    t_idx = result.rfind(\"Text:\")\n",
    "    if t_idx >= idx:\n",
    "        return \"\"\n",
    "    if idx != -1:\n",
    "        return result[idx+7:] + '\\n'\n",
    "    return \"\"   "
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-13T22:04:58.214209Z",
     "iopub.execute_input": "2024-04-13T22:04:58.214429Z",
     "iopub.status.idle": "2024-04-13T22:04:58.223684Z",
     "shell.execute_reply.started": "2024-04-13T22:04:58.214407Z",
     "shell.execute_reply": "2024-04-13T22:04:58.222948Z"
    },
    "trusted": true
   },
   "execution_count": 19,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def summarize_context(context):\n",
    "    prompt = \"\"\"Summarize the given text. Follow the examples for formatting.\n",
    "    \n",
    "    Text: Heteroskedasticity is a condition in which the variance of a data series is not constant. This can cause problems when performing statistical analysis, as it can lead to biased and inefficient results.\n",
    "    Answer: \n",
    "     Heteroskedasticity is a condition in which the variance of a data series is not constant.\n",
    "    \n",
    "    Text: {text}\n",
    "    Answer:\n",
    "    \"\"\"\n",
    "    p = prompt.format(\n",
    "        text=context,\n",
    "    )\n",
    "    \n",
    "    result = gemma_lm.generate(p)\n",
    "    idx = result.rfind(\"Answer:\")\n",
    "    t_idx = result.rfind(\"Text:\")\n",
    "    if t_idx >= idx:\n",
    "        return \"\"\n",
    "    if idx != -1:\n",
    "        return result[idx+7:] + '\\n'\n",
    "    return \"\""
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-13T22:04:58.224609Z",
     "iopub.execute_input": "2024-04-13T22:04:58.224857Z",
     "iopub.status.idle": "2024-04-13T22:04:58.237250Z",
     "shell.execute_reply.started": "2024-04-13T22:04:58.224825Z",
     "shell.execute_reply": "2024-04-13T22:04:58.236440Z"
    },
    "trusted": true
   },
   "execution_count": 20,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def get_answer(query):\n",
    "    # Input scoping\n",
    "    scope = \"In Data Science domain, \"\n",
    "    docs = retriever.get_relevant_documents(query=scope+query)\n",
    "    context = ''\n",
    "    for d in docs:\n",
    "        summary = summarize_context(d.metadata['summary'])        \n",
    "        context += summary\n",
    "        \n",
    "    keywords = extract_key_words(query)\n",
    "    context += keywords\n",
    "    prompt = template.format(\n",
    "        context=context,\n",
    "        question= scope + query,\n",
    "    )\n",
    "    # output sanitize?\n",
    "    return gemma_lm.generate(prompt)\n",
    "    "
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-13T22:04:58.238096Z",
     "iopub.execute_input": "2024-04-13T22:04:58.238313Z",
     "iopub.status.idle": "2024-04-13T22:04:58.250960Z",
     "shell.execute_reply.started": "2024-04-13T22:04:58.238291Z",
     "shell.execute_reply": "2024-04-13T22:04:58.250155Z"
    },
    "trusted": true
   },
   "execution_count": 21,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Result on fine-tuning with kaggle data only. \n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "qlist= [\"What is Data Science?\",\n",
    "\"Differentiate between Data Analytics and Data Science\",\n",
    "\"What are the differences between supervised and unsupervised learning?\",\n",
    "\"Explain the steps in making a decision tree.\",\n",
    "\"Differentiate between univariate, bivariate, and multivariate analysis.\",\n",
    "\"How should you maintain a deployed model?\",\n",
    "\"What is a Confusion Matrix?\",\n",
    "\"How is logistic regression done?\",\n",
    "\"What is the significance of p-value?\",\n",
    "\"Mention some techniques used for sampling.\"]\n",
    "\n",
    "for q in qlist:\n",
    "    print(get_answer(q))\n",
    "    print(\"\\n--------------\\n\")"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-13T22:04:58.251975Z",
     "iopub.execute_input": "2024-04-13T22:04:58.252239Z",
     "iopub.status.idle": "2024-04-13T22:06:39.685673Z",
     "shell.execute_reply.started": "2024-04-13T22:04:58.252216Z",
     "shell.execute_reply": "2024-04-13T22:06:39.684817Z"
    },
    "trusted": true
   },
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "text": "You are a data scientist that answer questions in data science domain in an easy to understand and eloquent way. Use the given Context as reference if it is relevant to the question.\n\nContext: \n    A data scientist is a professional who creates programming code and combines it with statistical knowledge to create insights from data.\n\n    Data science is a broad field of study that encompasses many different topics, including statistics, mathematics, computer science, and social science. Data science is a rapidly growing field, and new technologies and tools are being developed all the time.\n\n**Additional Notes:**\n\n* The text also mentions Matplotlib, which is a Python library for creating plots.\n* The text also mentions NumPy, which is a Python library for numerical computing.\n* The text also mentions Seaborn, which is a Python library for creating data visualizations.\n\n\nQuestion: In Data Science domain, What is Data Science?\n\nAnswer: \nData science is a broad field of study that encompasses many different topics, including statistics, mathematics, computer science, and social science. Data science is a rapidly growing field, and new technologies and tools are being developed all the time.\n\n--------------\n\nYou are a data scientist that answer questions in data science domain in an easy to understand and eloquent way. Use the given Context as reference if it is relevant to the question.\n\nContext: \n    Data science is a multidisciplinary field that combines many different skills and techniques to analyze data. It is a rapidly growing field that is used in many different industries, including business, government, and healthcare.\n\n\nQuestion: In Data Science domain, Differentiate between Data Analytics and Data Science\n\nAnswer: \nSure, here's the difference between Data Analytics and Data Science:\n\n**Data Analytics** is the process of collecting, cleaning, and analyzing data to extract meaningful insights. Data analytics is a subset of data science that focuses on the analysis of data to extract meaningful insights.\n\n**Data Science** is the process of using a systematic approach to collect, analyze, and interpret data to solve a specific problem or achieve a specific goal. Data science is a broader field that encompasses data analytics, but also includes other techniques such as machine learning, data mining, and data visualization.\n\n**Here's an analogy:**\n\nThink of data analytics as a chef preparing a meal using a set of ingredients. Data science is like a chef who is planning a multi-course meal using a variety of ingredients and techniques.\n\n--------------\n\nYou are a data scientist that answer questions in data science domain in an easy to understand and eloquent way. Use the given Context as reference if it is relevant to the question.\n\nContext: \n    Weak supervision is a paradigm in machine learning, the relevance and notability of which increased with the advent of large language models due to large amount of data required to train them.\n\n    Active learning is a special case of machine learning in which a learning algorithm can interactively query a human user (or some other information source), to label new data points with the desired outputs.\n\n    Supervised learning is a type of machine learning where the data is labeled with a target variable. Unsupervised learning is a type of machine learning where the data is not labeled with a target variable.\n\n\nQuestion: In Data Science domain, What are the differences between supervised and unsupervised learning?\n\nAnswer: \nSure, here's the difference between supervised and unsupervised learning:\n\n**Supervised Learning:**\n\n* In supervised learning, the data is labeled with a target variable.\n* The learning algorithm uses this labeled data to learn the relationship between the input and output variables.\n* Once the learning algorithm has been trained, it can be used to make predictions on new data points.\n\n**Unsupervised Learning:**\n\n* In unsupervised learning, the data is not labeled with a target variable.\n* The learning algorithm uses this unlabeled data to find patterns and relationships in the data.\n* Once the learning algorithm has been trained, it can be used to make predictions on new data points.\n\n--------------\n\nYou are a data scientist that answer questions in data science domain in an easy to understand and eloquent way. Use the given Context as reference if it is relevant to the question.\n\nContext: \n    Automated decision-making involves the use of data, machines and algorithms to make decisions in a range of contexts, including public administration, business, health, education, law, employment, transport, media and entertainment, with varying degrees of human oversight or intervention.\n\n    Data analysis is the process of inspecting, cleansing, transforming, and modeling data with the goal of discovering useful information, informing conclusions, and supporting decision-making.\n\n    Decision tree is a supervised machine learning algorithm used to make predictions on a target variable. The decision tree algorithm works by iteratively splitting the data into subsets based on the value of the target variable. The algorithm then builds a tree that makes predictions on the target variable.\n\n\nQuestion: In Data Science domain, Explain the steps in making a decision tree.\n\nAnswer: \nSure, here are the steps involved in making a decision tree:\n\n1. **Gather and Clean Data:**\n    - Collect data from a source that contains the target variable and other relevant variables.\n    - Check for missing values and outliers.\n    - Transform the data to make it suitable for modeling.\n\n2. **Feature Engineering:**\n    - Identify the target variable and other relevant variables.\n    - Create new variables by combining or transforming existing variables.\n\n3. **Split the Data:**\n    - Divide the data into training and testing sets.\n    - The training set is used to build the decision tree, while the testing set is used to evaluate the tree's performance.\n\n4. **Build the Decision Tree:**\n    - Start with a root node.\n    - For each iteration, split the data based on the most important feature.\n    - Continue splitting until the tree reaches a leaf node.\n\n5. **Evaluate the Decision Tree:**\n    - Use the testing set to evaluate the tree's performance.\n    - Metrics such as accuracy, precision, and recall can be used to evaluate the tree's performance.\n\n6. **Refine the Decision Tree:**\n    - If the tree's performance is not satisfactory, refine it by changing the split criteria or the number of iterations.\n    - Continue refining until the tree's performance is satisfactory.\n\n7. **Use the Decision Tree:**\n    - Once the decision tree is finished, use\n\n--------------\n\nYou are a data scientist that answer questions in data science domain in an easy to understand and eloquent way. Use the given Context as reference if it is relevant to the question.\n\nContext: \n    Polynomials are mathematical expressions consisting of indeterminates and coefficients, that involve only the operations of addition, subtraction, multiplication, and positive-integer powers of variables.\n\n    Univariate analysis is a statistical method that is used to analyze data that has only one variable. Bivariate analysis is a statistical method that is used to analyze data that has two variables. Multivariate analysis is a statistical method that is used to analyze data that has more than two variables.\n\n\nQuestion: In Data Science domain, Differentiate between univariate, bivariate, and multivariate analysis.\n\nAnswer: \nSure, here's a differentiation between univariate, bivariate, and multivariate analysis:\n\n**Univariate Analysis**\n\nUnivariate analysis is a statistical method that is used to analyze data that has only one variable. The goal of univariate analysis is to identify patterns and trends in the data. This can be done by plotting the data on a scatter plot, or by using a statistical software package to perform a regression analysis.\n\n**Bivariate Analysis**\n\nBivariate analysis is a statistical method that is used to analyze data that has two variables. The goal of bivariate analysis is to identify relationships between the two variables. This can be done by plotting the data on a scatter plot, or by using a statistical software package to perform a correlation analysis.\n\n**Multivariate Analysis**\n\nMultivariate analysis is a statistical method that is used to analyze data that has more than two variables. The goal of multivariate analysis is to identify relationships between all of the variables in the data. This can be done by plotting the data on a scatter plot, or by using a statistical software package to perform a principal component analysis (PCA).\n\n--------------\n\nYou are a data scientist that answer questions in data science domain in an easy to understand and eloquent way. Use the given Context as reference if it is relevant to the question.\n\nContext: \n    Performance testing is a computer science practice which strives to build performance standards into the implementation, design and architecture of a system.\n\n    A deployed model is a model that is already running and accessible to users.\n\n\nQuestion: In Data Science domain, How should you maintain a deployed model?\n\nAnswer: \nTo maintain a deployed model, you should follow these steps:\n\n1. **Monitor the model's performance.** This can be done by using a monitoring tool to track metrics such as latency, throughput, and errors.\n2. **Identify any issues with the model.** If you find any issues, fix them immediately.\n3. **Restart the model.** Restarting the model can help to fix any issues that you have identified.\n4. **Monitor the model's performance again.** Once you have restarted the model, monitor its performance to make sure that it is still performing as expected.\n5. **Repeat steps 1-4 as needed.** If you find any issues with the model, repeat steps 1-4 to fix them.\n\n--------------\n\nYou are a data scientist that answer questions in data science domain in an easy to understand and eloquent way. Use the given Context as reference if it is relevant to the question.\n\nContext: \n    Principal component analysis (PCA) is a linear dimensionality reduction technique with applications in exploratory data analysis, visualization and data preprocessing.\n\n    A confusion matrix is a visualization tool used to compare the predicted labels of a dataset with the actual labels. It is used to evaluate the quality of a classification model.\n\n\nQuestion: In Data Science domain, What is a Confusion Matrix?\n\nAnswer: \nA confusion matrix is a visualization tool used to compare the predicted labels of a dataset with the actual labels. It is used to evaluate the quality of a classification model.\n\n--------------\n\nYou are a data scientist that answer questions in data science domain in an easy to understand and eloquent way. Use the given Context as reference if it is relevant to the question.\n\nContext: \n    Regression analysis is primarily used for prediction and forecasting, where its use has substantial overlap with the field of machine learning. In some situations regression analysis can be used to infer causal relationships between the independent and dependent variables.\n\n    Logistic regression is a statistical method used to predict the probability of an event occurring.\n\n\nQuestion: In Data Science domain, How is logistic regression done?\n\nAnswer: \nSure, here's a breakdown of how logistic regression is done in the context of data science domain:\n\n1. **Data Preparation:**\n    - Gather data on the independent and dependent variables.\n    - Ensure that the data is clean and free of errors.\n\n2. **Feature Scaling:**\n    - Scale the independent and dependent variables to a similar range.\n    - This helps to ensure that the model is not biased by the distribution of the independent variables.\n\n3. **Model Selection:**\n    - Choose a suitable model for the data.\n    - In this case, the most suitable model is logistic regression.\n\n4. **Model Training:**\n    - Train the model on the data.\n    - This involves finding the best set of coefficients that minimizes the mean squared error between the predicted and actual values.\n\n5. **Model Evaluation:**\n    - Evaluate the model on a separate test set.\n    - This involves comparing the model's predictions to the actual values.\n    - The model's performance is measured by metrics such as accuracy, precision, and recall.\n\n6. **Model Interpretation:**\n    - Analyze the model's coefficients to understand how they affect the dependent variable.\n    - This can help to identify the most important independent variables and to make predictions about the dependent variable.\n\n7. **Prediction:**\n    - Use the trained model to make predictions on new data.\n    - The model can be used to predict the probability of an event occurring.\n\n--------------\n\nYou are a data scientist that answer questions in data science domain in an easy to understand and eloquent way. Use the given Context as reference if it is relevant to the question.\n\nContext: \n    The p{\\displaystyle p}-value is the probability of obtaining test results at least as extreme as the result actually observed, under the assumption that the null hypothesis is correct.\n\n    The term significance does not imply importance here, and the term statistical significance is not the same as research significance, theoretical significance, or practical significance.\n\n    Significance p-value is a measure of how statistically significant a difference between two groups of data is.\n\n\nQuestion: In Data Science domain, What is the significance of p-value?\n\nAnswer: \nThe significance of p-value is the probability of obtaining test results at least as extreme as the result actually observed, under the assumption that the null hypothesis is correct.\n\n--------------\n\nYou are a data scientist that answer questions in data science domain in an easy to understand and eloquent way. Use the given Context as reference if it is relevant to the question.\n\nContext: \n    Sampling is the selection of a subset or a statistical sample (termed sample for short) of individuals from within a statistical population to estimate characteristics of the whole population.\n\n    Data analysis is the process of inspecting, cleansing, transforming, and modeling data with the goal of discovering useful information, informing conclusions, and supporting decision-making.\n\n    Sampling is a technique used to select a subset of data that is representative of the whole dataset. Sampling techniques can be used to improve the efficiency of data analysis.\n\n\nQuestion: In Data Science domain, Mention some techniques used for sampling.\n\nAnswer: \nSure, here are some techniques used for sampling in Data Science domain:\n\n- Simple random sampling: This technique involves selecting a subset of data randomly. Simple random sampling is a simple and efficient technique that can be used to select a subset of data that is representative of the whole dataset.\n\n\n- Stratified sampling: This technique involves dividing the data into strata, or subgroups, based on shared characteristics. Stratified sampling is a technique that can be used to select a subset of data that is representative of the whole dataset.\n\n\n- Cluster sampling: This technique involves dividing the data into clusters, or groups, based on shared characteristics. Cluster sampling is a technique that can be used to select a subset of data that is representative of the whole dataset.\n\n\n- Systematic sampling: This technique involves selecting a subset of data systematically, or in a predefined order. Systematic sampling is a technique that can be used to select a subset of data that is representative of the whole dataset.\n\n--------------\n\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Result on fine-tuning with kaggle data + qa data. \n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    " gemma_lm.fit(qa_data, epochs=3, batch_size=1)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-13T22:06:39.686659Z",
     "iopub.execute_input": "2024-04-13T22:06:39.686924Z",
     "iopub.status.idle": "2024-04-13T22:07:16.716156Z",
     "shell.execute_reply.started": "2024-04-13T22:06:39.686897Z",
     "shell.execute_reply": "2024-04-13T22:07:16.715350Z"
    },
    "trusted": true
   },
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "text": "Epoch 1/3\n\u001B[1m81/81\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m12s\u001B[0m 150ms/step - loss: 0.1946 - sparse_categorical_accuracy: 0.6585\nEpoch 2/3\n\u001B[1m81/81\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m12s\u001B[0m 150ms/step - loss: 0.1672 - sparse_categorical_accuracy: 0.6872\nEpoch 3/3\n\u001B[1m81/81\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m12s\u001B[0m 150ms/step - loss: 0.1451 - sparse_categorical_accuracy: 0.6946\n",
     "output_type": "stream"
    },
    {
     "execution_count": 23,
     "output_type": "execute_result",
     "data": {
      "text/plain": "<keras.src.callbacks.history.History at 0x7cbfd20ffcd0>"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "for q in qlist:\n",
    "    print(get_answer(q))\n",
    "    print(\"\\n--------------\\n\")"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-13T22:07:16.717053Z",
     "iopub.execute_input": "2024-04-13T22:07:16.717291Z",
     "iopub.status.idle": "2024-04-13T22:08:22.232122Z",
     "shell.execute_reply.started": "2024-04-13T22:07:16.717267Z",
     "shell.execute_reply": "2024-04-13T22:08:22.230806Z"
    },
    "trusted": true
   },
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "text": "You are a data scientist that answer questions in data science domain in an easy to understand and eloquent way. Use the given Context as reference if it is relevant to the question.\n\nContext: \n    Data science is an interdisciplinary academic field that uses statistics, scientific computing, scientific methods, processes, algorithms and systems to extract or extrapolate knowledge and insights from potentially noisy, structured, or unstructured data.\n\n    Machine learning is a subfield of data science that focuses on algorithms that can learn from data. Machine learning algorithms are used to solve problems that are too complex for traditional statistical methods.\n\n\nQuestion: In Data Science domain, What is Data Science?\n\nAnswer: \nData science is an interdisciplinary academic field that uses statistics, scientific computing, scientific methods, processes, algorithms and systems to extract or extrapolate knowledge and insights from potentially noisy, structured, or unstructured data.\n\n--------------\n\nYou are a data scientist that answer questions in data science domain in an easy to understand and eloquent way. Use the given Context as reference if it is relevant to the question.\n\nContext: \n    The difference between Data Analytics and Data Science is that Data Analytics focuses on the process of collecting, cleaning, and analyzing data, while Data Science focuses on the use of data to solve real-world problems.\n\n\nQuestion: In Data Science domain, Differentiate between Data Analytics and Data Science\n\nAnswer: \nSure, here's the difference between Data Analytics and Data Science:\n\n**Data Analytics:**\n\n* Focuses on the process of collecting, cleaning, and analyzing data.\n* Uses statistical methods and techniques to identify patterns and trends in data.\n* Provides insights and recommendations based on data analysis.\n* Often involves tasks such as data wrangling, data exploration, and data visualization.\n\n**Data Science:**\n\n* Focuses on the use of data to solve real-world problems.\n* Uses a broader range of methods and techniques, including machine learning, data mining, and statistical analysis.\n* Provides actionable insights and solutions to decision-makers.\n* Often involves tasks such as data wrangling, data exploration, data modeling, and data visualization.\n\nIn summary, Data Analytics focuses on the process of collecting, cleaning, and analyzing data, while Data Science focuses on the use of data to solve real-world problems.\n\n--------------\n\nYou are a data scientist that answer questions in data science domain in an easy to understand and eloquent way. Use the given Context as reference if it is relevant to the question.\n\nContext: \n    Weak supervision is a paradigm in machine learning, the relevance and notability of which increased with the advent of large language models due to large amount of data required to train them.\n\n    Active learning is a special case of machine learning in which a learning algorithm can interactively query a human user (or some other information source), to label new data points with the desired outputs.\n\n    Supervised learning is a type of machine learning where the training data is labeled with the target variable. Unsupervised learning is a type of machine learning where the training data is not labeled with the target variable.\n\n\nQuestion: In Data Science domain, What are the differences between supervised and unsupervised learning?\n\nAnswer: \nSupervised learning is a type of machine learning where the training data is labeled with the target variable. Unsupervised learning is a type of machine learning where the training data is not labeled with the target variable.\n\n--------------\n\nYou are a data scientist that answer questions in data science domain in an easy to understand and eloquent way. Use the given Context as reference if it is relevant to the question.\n\nContext: \n    Automated decision-making involves the use of data, machines and algorithms to make decisions in a range of contexts, including public administration, business, health, education, law, employment, transport, media and entertainment, with varying degrees of human oversight or intervention.\n\n    Data analysis is the process of inspecting, cleansing, transforming, and modeling data with the goal of discovering useful information, informing conclusions, and supporting decision-making.\n\n    Decision tree is a supervised machine learning algorithm used for classification and regression tasks. It is a tree-structured model that splits data into subsets based on the value of a feature. The decision tree algorithm is used to build a model that can predict the target variable from the input variables.\n\n\nQuestion: In Data Science domain, Explain the steps in making a decision tree.\n\nAnswer: \nSure, here are the steps involved in making a decision tree:\n\n1. **Gather and Preprocess Data**: Collect and clean the data that you want to use to build the decision tree. This includes data cleaning, feature scaling, and feature selection.\n\n\n2. **Feature Engineering**: Create new features from existing features to improve the performance of the decision tree.\n\n\n3. **Decision Tree Algorithm**: Train a decision tree algorithm on the data. The decision tree algorithm will create a tree structure that splits the data into subsets based on the value of a feature.\n\n\n4. **Feature Selection**: Select the most important features to use in the decision tree. Feature selection can be done using various methods, such as correlation analysis, feature importance metrics, and grid search.\n\n\n5. **Model Evaluation**: Evaluate the decision tree model using cross-validation or k-fold cross-validation. Cross-validation helps to prevent overfitting and improve the model'sgeneralizability.\n\n\n6. **Model Refinement**: Refine the decision tree model by adjusting the parameters and splitting criteria. Refinement can be done using grid search, cross-validation, or other methods.\n\n\n7. **Model Deployment**: Deploy the decision tree model to a production system. This can be done using a variety of methods, such as deploying the model as a web service or embedding the model into a decision support system.\n\n--------------\n\nYou are a data scientist that answer questions in data science domain in an easy to understand and eloquent way. Use the given Context as reference if it is relevant to the question.\n\nContext: \n    Polynomials are mathematical expressions consisting of indeterminates and coefficients, that involve only the operations of addition, subtraction, multiplication, and positive-integer powers of variables.\n\n    Univariate analysis is a statistical technique used to analyze data that has only one variable. Bivariate analysis is a statistical technique used to analyze data that has two variables. Multivariate analysis is a statistical technique used to analyze data that has more than two variables.\n\n\nQuestion: In Data Science domain, Differentiate between univariate, bivariate, and multivariate analysis.\n\nAnswer: \nSure, here's a differentiation between univariate, bivariate, and multivariate analysis:\n\n**Univariate Analysis:**\nUnivariate analysis is a statistical technique used to analyze data that has only one variable. The goal of univariate analysis is to identify patterns and trends in the data.\n\n**Bivariate Analysis:**\nBivariate analysis is a statistical technique used to analyze data that has two variables. The goal of bivariate analysis is to identify relationships between the two variables.\n\n**Multivariate Analysis:**\nMultivariate analysis is a statistical technique used to analyze data that has more than two variables. The goal of multivariate analysis is to identify relationships between all of the variables in the data.\n\n--------------\n\nYou are a data scientist that answer questions in data science domain in an easy to understand and eloquent way. Use the given Context as reference if it is relevant to the question.\n\nContext: \n    Performance testing is a computer science practice which strives to build performance standards into the implementation, design and architecture of a system.\n\n    Maintain deployed model refers to the process of saving the trained model to a file so that it can be used later.\n\n\nQuestion: In Data Science domain, How should you maintain a deployed model?\n\nAnswer: \nMaintain deployed model refers to the process of saving the trained model to a file so that it can be used later.\n\n--------------\n\nYou are a data scientist that answer questions in data science domain in an easy to understand and eloquent way. Use the given Context as reference if it is relevant to the question.\n\nContext: \n    Principal component analysis (PCA) is a linear dimensionality reduction technique with applications in exploratory data analysis, visualization and data preprocessing.\n\n    A confusion matrix is a visualization tool used to assess the performance of a classification model. It shows the true positives, false positives, true negatives, and false negatives in a dataset.\n\n\nQuestion: In Data Science domain, What is a Confusion Matrix?\n\nAnswer: \nA confusion matrix is a visualization tool used to assess the performance of a classification model. It shows the true positives, false positives, true negatives, and false negatives in a dataset.\n\n--------------\n\nYou are a data scientist that answer questions in data science domain in an easy to understand and eloquent way. Use the given Context as reference if it is relevant to the question.\n\nContext: \n    Regression analysis is primarily used for prediction and forecasting, where its use has substantial overlap with the field of machine learning. In some situations regression analysis can be used to infer causal relationships between the independent and dependent variables.\n\n    Logistic regression is a statistical method used to predict the probability of an event occurring.\n\n\nQuestion: In Data Science domain, How is logistic regression done?\n\nAnswer: \nLogistic regression is a statistical method used to predict the probability of an event occurring. Logistic regression is used to predict the probability of an event occurring by fitting a linear model to a set of data. The model is then used to predict the probability of an event occurring for a given set of data.\n\n--------------\n\nYou are a data scientist that answer questions in data science domain in an easy to understand and eloquent way. Use the given Context as reference if it is relevant to the question.\n\nContext: \n    The ASA statement is that p-values do not measure the probability that the studied hypothesis is true, or the probability that the data were produced by random chance alone.\n\n    The term significance does not imply importance here, and the term statistical significance is not the same as research significance, theoretical significance, or practical significance.\n\n    Significance p-value is a measure of how statistically significant a difference between two groups of data is.\n\n\nQuestion: In Data Science domain, What is the significance of p-value?\n\nAnswer: \nThe significance of p-value is a measure of how statistically significant a difference between two groups of data is.\n\n--------------\n\nYou are a data scientist that answer questions in data science domain in an easy to understand and eloquent way. Use the given Context as reference if it is relevant to the question.\n\nContext: \n    Sampling is the selection of a subset or a statistical sample (termed sample for short) of individuals from within a statistical population to estimate characteristics of the whole population.\n\n    Data analysis is the process of inspecting, cleansing, transforming, and modeling data with the goal of discovering useful information, informing conclusions, and supporting decision-making.\n\n\nQuestion: In Data Science domain, Mention some techniques used for sampling.\n\nAnswer: \nSure, here are some techniques used for sampling in Data Science domain:\n\n* Simple random sampling: This technique involves selecting individuals from the population randomly without replacement.\n* Systematic sampling: This technique involves selecting individuals from the population in a systematic order, such as every nth individual.\n* Stratified sampling: This technique involves dividing the population into strata based on shared characteristics and then selecting individuals from each stratum proportionally.\n* Cluster sampling: This technique involves dividing the population into clusters based on shared characteristics and then selecting individuals from each cluster proportionally.\n* Purposive sampling: This technique involves selecting individuals based on their relevance to the study.\n\n--------------\n\n",
     "output_type": "stream"
    }
   ]
  }
 ]
}
